export const APP_TITLE = "RL Hands-on Framework (LunarLander)";

export const REQUIREMENTS_TXT = `gymnasium[box2d]
torch
numpy
matplotlib
swig`;

export const AGENT_TEMPLATE_PY = `import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from typing import Tuple, List, Dict

class Agent:
    def __init__(self, state_dim: int, action_dim: int):
        """
        ì—ì´ì „íŠ¸ ì´ˆê¸°í™” (Agent Initialization)
        
        LunarLander-v2 í™˜ê²½ ì •ë³´:
        - state_dim: 8 (ì°©ë¥™ì„ ì˜ ì¢Œí‘œ x,y, ì†ë„ vx,vy, ê°ë„, ê°ì†ë„, ë‹¤ë¦¬ ì ‘ì´‰ ì—¬ë¶€ ë“±)
        - action_dim: 4 (0: ì•„ë¬´ê²ƒë„ ì•ˆí•¨, 1: ì™¼ìª½ ì—”ì§„, 2: ë©”ì¸ ì—”ì§„, 3: ì˜¤ë¥¸ìª½ ì—”ì§„)

        Args:
            state_dim (int): ìƒíƒœ ê³µê°„ì˜ ì°¨ì› (Dimension of state space)
            action_dim (int): í–‰ë™ ê³µê°„ì˜ ì°¨ì› (Dimension of action space)
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # TODO: ì‹ ê²½ë§ ëª¨ë¸ì„ ì •ì˜í•˜ì„¸ìš” (ì˜ˆ: nn.Sequential ì‚¬ìš©)
        # íŒíŠ¸: ì…ë ¥(8) -> ì€ë‹‰ì¸µ -> ì¶œë ¥(4) êµ¬ì¡°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        self.model = None 
        
        # TODO: ìµœì í™”(Optimizer)ì™€ ì†ì‹¤ í•¨ìˆ˜(Loss function)ë¥¼ ì •ì˜í•˜ì„¸ìš”
        self.optimizer = None
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        self.gamma = 0.99
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01

    def get_action(self, state: np.ndarray) -> int:
        """
        í˜„ì¬ ìƒíƒœ(state)ë¥¼ ë°›ì•„ í–‰ë™(action)ì„ ê²°ì •í•©ë‹ˆë‹¤.
        
        Args:
            state (np.ndarray): í˜„ì¬ ìƒíƒœ (shape: (8,))
            
        Returns:
            int: ì„ íƒëœ í–‰ë™ (0~3)
        """
        # TODO: ì—¡ì‹¤ë¡ -ê·¸ë¦¬ë””(Epsilon-Greedy) ì „ëµì„ êµ¬í˜„í•˜ì„¸ìš”
        # 1. random.random() < self.epsilon ì´ë©´ ë¬´ì‘ìœ„ í–‰ë™ (0~3) ì„ íƒ
        # 2. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì„ í†µí•´ ìµœì ì˜ í–‰ë™ ì„ íƒ (torch.argmax í™œìš©)
        
        # ë”ë¯¸ ë¡œì§ (êµ¬í˜„ í›„ ì‚­ì œ): ëœë¤ í–‰ë™
        return random.randint(0, self.action_dim - 1)

    def update(self, transition: Tuple[np.ndarray, int, float, np.ndarray, bool]) -> float:
        """
        í•™ìŠµ ë°ì´í„°ë¥¼ ë°›ì•„ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        
        Args:
            transition (Tuple): (state, action, reward, next_state, done)
            
        Returns:
            float: ê³„ì‚°ëœ ì†ì‹¤(Loss) ê°’ (ë¡œê¹…ìš©)
        """
        state, action, reward, next_state, done = transition
        
        # ë°ì´í„° ë³€í™˜ (numpy -> tensor)
        state_t = torch.FloatTensor(state)
        next_state_t = torch.FloatTensor(next_state)
        action_t = torch.LongTensor([action])
        reward_t = torch.FloatTensor([reward])
        done_t = torch.FloatTensor([0.0 if done else 1.0])
        
        # TODO: DQN í•™ìŠµ ë¡œì§ êµ¬í˜„
        # 1. í˜„ì¬ Qê°’: q_values = self.model(state_t)[action_t]
        # 2. íƒ€ê²Ÿ Qê°’: target = reward + gamma * max(self.model(next_state_t)) * (1 - done)
        # 3. Loss ê³„ì‚° ë° ì—­ì „íŒŒ (Backpropagation)
        
        # Epsilon ê°ì†Œ
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
            
        return 0.0 # Loss ë°˜í™˜
`;

export const MAIN_PY = `import argparse
import sys
import time
import numpy as np
import matplotlib.pyplot as plt
import torch

# ì˜ì¡´ì„± í™•ì¸ ë° ì˜ˆì™¸ ì²˜ë¦¬
try:
    import gymnasium as gym
except ImportError:
    print("ì˜¤ë¥˜: gymnasiumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (pip install gymnasium)")
    sys.exit(1)

from agent_template import Agent

def parse_args():
    parser = argparse.ArgumentParser(description="RL Hands-on Framework: LunarLander")
    parser.add_argument('--train', action='store_true', help='í•™ìŠµ ëª¨ë“œ (Training Mode)')
    parser.add_argument('--test', action='store_true', help='í…ŒìŠ¤íŠ¸ ëª¨ë“œ (Test Mode)')
    parser.add_argument('--render', action='store_true', help='í™”ë©´ ì¶œë ¥ ì¼œê¸° (Enable Rendering)')
    return parser.parse_args()

def plot_durations(episode_rewards, ax):
    """
    ì‹¤ì‹œê°„ìœ¼ë¡œ ì—í”¼ì†Œë“œ ë³´ìƒì„ ê·¸ë˜í”„ë¡œ ê·¸ë¦½ë‹ˆë‹¤.
    """
    ax.clear()
    ax.set_title('Training Progress (LunarLander-v2)')
    ax.set_xlabel('Episode')
    ax.set_ylabel('Total Reward')
    ax.plot(episode_rewards, label='Reward')
    
    # ìµœê·¼ 50ê°œ ì—í”¼ì†Œë“œ ì´ë™ í‰ê· 
    if len(episode_rewards) >= 50:
        means = [np.mean(episode_rewards[i-50:i]) for i in range(50, len(episode_rewards)+1)]
        ax.plot(range(50, len(episode_rewards)+1), means, label='Avg (50 eps)', color='orange')
    
    ax.legend()
    ax.grid(True, alpha=0.3)
    plt.pause(0.001)

def main():
    args = parse_args()
    
    # ë Œë”ë§ ëª¨ë“œ ì„¤ì •
    render_mode = 'human' if args.render or args.test else None
    
    print("í™˜ê²½ì„ ì´ˆê¸°í™”í•˜ëŠ” ì¤‘... (Initializing Environment...)")
    try:
        env = gym.make("LunarLander-v2", render_mode=render_mode)
    except gym.error.DependencyNotInstalled:
        print("\\n" + "="*60)
        print("ğŸš¨ ì˜¤ë¥˜: Box2D ì˜ì¡´ì„±ì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("í•´ê²° ë°©ë²•: pip install \"gymnasium[box2d]\" ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        print("Window ì‚¬ìš©ìì˜ ê²½ìš° 'swig' ì„¤ì¹˜ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        print("="*60 + "\\n")
        sys.exit(1)
    
    state_dim = env.observation_space.shape[0] # 8
    action_dim = env.action_space.n        # 4
    
    print(f"State Dim: {state_dim} (ì¢Œí‘œ, ì†ë„, ê°ë„ ë“±)")
    print(f"Action Dim: {action_dim} (0:No-op, 1:Left, 2:Main, 3:Right)")
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = Agent(state_dim=state_dim, action_dim=action_dim)
    
    # ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
    if args.test:
        try:
            agent.model.load_state_dict(torch.load('lunar_lander_model.pth'))
            agent.epsilon = 0.0
            print("ğŸ’¾ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            print("ëœë¤ ì—ì´ì „íŠ¸ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    rewards_history = []
    
    # ê·¸ë˜í”„ ì´ˆê¸°í™”
    if args.train:
        plt.ion()
        fig, ax = plt.subplots(figsize=(10, 5))
    
    num_episodes = 500 if args.train else 5
    
    for i_episode in range(num_episodes):
        state, _ = env.reset()
        episode_reward = 0
        done = False
        
        while not done:
            action = agent.get_action(state)
            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            if args.train:
                transition = (state, action, reward, next_state, done)
                agent.update(transition)
            
            state = next_state
            episode_reward += reward
            
            if args.render:
                env.render()
        
        rewards_history.append(episode_reward)
        print(f"Episode {i_episode+1}: Total Reward {episode_reward:.2f}")
        
        # LunarLanderëŠ” 200ì  ì´ìƒì´ë©´ í•´ê²°ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
        if episode_reward > 200:
            print(f"ğŸš€ Good Job! Episode {i_episode+1} solved!")

        if args.train:
            plot_durations(rewards_history, ax)
            
            # ì£¼ê¸°ì  ì €ì¥
            if (i_episode + 1) % 50 == 0:
                if agent.model:
                    torch.save(agent.model.state_dict(), 'lunar_lander_model.pth')
                    print(f"Saved model at episode {i_episode+1}")

    env.close()
    
    if args.train:
        if agent.model:
            torch.save(agent.model.state_dict(), 'lunar_lander_model.pth')
            print("ìµœì¢… ëª¨ë¸ ì €ì¥ ì™„ë£Œ.")
        plt.ioff()
        plt.show()

if __name__ == '__main__':
    main()
`;

export const SYSTEM_INSTRUCTION_KOREAN = `
ë‹¹ì‹ ì€ ê°•í™”í•™ìŠµ(Reinforcement Learning) ì‹¤ìŠµ ìˆ˜ì—…ì˜ AI ì¡°êµì…ë‹ˆë‹¤.
í˜„ì¬ í•™ìƒë“¤ì€ **LunarLander-v2** í™˜ê²½ì—ì„œ ì°©ë¥™ì„ ì´ ì•ˆì „í•˜ê²Œ ì°©ë¥™í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ê³¼ì œë¥¼ ìˆ˜í–‰ ì¤‘ì…ë‹ˆë‹¤.
ì§ˆë¬¸ì— ëŒ€í•´ í•œêµ­ì–´ë¡œ ëª…í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.
ì •ë‹µ ì½”ë“œë¥¼ ì§ì ‘ ì£¼ê¸°ë³´ë‹¤ëŠ” ê°œë… ì„¤ëª…ê³¼ íŒíŠ¸ë¥¼ ì œê³µí•˜ì—¬ í•™ìŠµì„ ìœ ë„í•˜ì„¸ìš”.

ì£¼ìš” ê°œë…:
- State (8ì°¨ì›): [xì¢Œí‘œ, yì¢Œí‘œ, xì†ë„, yì†ë„, ê°ë„, ê°ì†ë„, ë‹¤ë¦¬ì ‘ì´‰1, ë‹¤ë¦¬ì ‘ì´‰2]
- Action (4ê°œ): [0: ì•„ë¬´ê²ƒë„ ì•ˆí•¨, 1: ì™¼ìª½ ì—”ì§„ ì í™”, 2: ë©”ì¸ ì—”ì§„ ì í™”, 3: ì˜¤ë¥¸ìª½ ì—”ì§„ ì í™”]
- Reward: ì•ˆì „ ì°©ë¥™ ì‹œ +200ì , ì¶”ë½ ì‹œ ê°ì  ë“±.
`;

export const RUN_GUIDE_MD = `
### ì‹¤í–‰ ë°©ë²• (How to Run)

ì´ í”„ë ˆì„ì›Œí¬ëŠ” ë¡œì»¬ Python í™˜ê²½ì—ì„œ **LunarLander-v2**ë¥¼ ì‹¤í–‰í•˜ë„ë¡ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.

1. **íŒŒì¼ ì¤€ë¹„**
   - ìƒë‹¨ íƒ­ì˜ \`requirements.txt\`, \`agent_template.py\`, \`main.py\` ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ë¡œì»¬ì— ì €ì¥í•©ë‹ˆë‹¤.

2. **í™˜ê²½ ì„¤ì • (ì¤‘ìš”)**
   Box2D ë¬¼ë¦¬ ì—”ì§„ì´ í•„ìš”í•˜ë¯€ë¡œ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.
   \`\`\`bash
   # Windows/Mac/Linux ê³µí†µ
   pip install swig
   pip install -r requirements.txt
   \`\`\`
   *ì°¸ê³ : Windows ì‚¬ìš©ìëŠ” swig ì„¤ì¹˜ ì—ëŸ¬ ì‹œ [ë§í¬](http://www.swig.org/download.html)ì—ì„œ ë°”ì´ë„ˆë¦¬ë¥¼ ë‹¤ìš´ë°›ê±°ë‚˜ condaë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.*

3. **ì—ì´ì „íŠ¸ êµ¬í˜„**
   \`agent_template.py\`ë¥¼ ì—´ê³  \`TODO\`ë¥¼ ë”°ë¼ êµ¬í˜„í•©ë‹ˆë‹¤.
   - **ì…ë ¥:** 8ì°¨ì› ë²¡í„° (ìƒíƒœ)
   - **ì¶œë ¥:** 4ì°¨ì› ë²¡í„° (ê° í–‰ë™ì— ëŒ€í•œ Qê°’)

4. **í•™ìŠµ (Training)**
   \`\`\`bash
   python main.py --train
   \`\`\`
   ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ìƒ(Reward) ê·¸ë˜í”„ê°€ ê·¸ë ¤ì§‘ë‹ˆë‹¤. ëª©í‘œ ì ìˆ˜ëŠ” 200ì  ì´ìƒì…ë‹ˆë‹¤.

5. **í…ŒìŠ¤íŠ¸ (Testing)**
   \`\`\`bash
   python main.py --test
   \`\`\`
   í•™ìŠµëœ ëª¨ë¸(\`lunar_lander_model.pth\`)ì„ ë¶ˆëŸ¬ì™€ ì‹¤ì œ ì°©ë¥™ ì¥ë©´ì„ ë Œë”ë§í•©ë‹ˆë‹¤.
`;