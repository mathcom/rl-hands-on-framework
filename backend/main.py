import os
import re
import logging
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Dict, Any

# LangChain Imports
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_ollama import OllamaEmbeddings, ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_core.documents import Document

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="RL Agent RAG Server")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- ì„¤ì • ìƒìˆ˜ ---
# OLLAMA_HOST í™˜ê²½ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ base_urlì€ ì½”ë“œì—ì„œ ì œê±°í•´ë„ ë˜ì§€ë§Œ, 
# ëª…ì‹œì ì¸ ì°¸ì¡°ë¥¼ ìœ„í•´ ë³€ìˆ˜ë§Œ ë‚¨ê²¨ë‘¡ë‹ˆë‹¤.
EMBEDDING_MODEL = "nomic-embed-text"
LLM_MODEL = "llama3.1"
DATA_PATH = "/app/data"

# ì „ì—­ ë³€ìˆ˜
retriever = None

def extract_from_constants_ts():
    """
    src/constants.ts íŒŒì¼ì„ ì½ì–´ ê·¸ ì•ˆì— í•˜ë“œì½”ë”©ëœ
    ì½”ë“œ í…œí”Œë¦¿ê³¼ ìš”êµ¬ì‚¬í•­ ë¬¸ìì—´ì„ ì¶”ì¶œí•˜ì—¬ Documentë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    # [ìˆ˜ì • 1] ë³€ìˆ˜ ì´ˆê¸°í™”ë¥¼ ë§¨ ìœ„ë¡œ ì˜¬ë¦¼ (Scope ì—ëŸ¬ ë°©ì§€)
    extracted_docs = []
    constants_path = os.path.join(DATA_PATH, "src/constants.ts")

    if not os.path.exists(constants_path):
        logger.warning(f"âš ï¸ constants.ts íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {constants_path}")
        return extracted_docs

    try:
        with open(constants_path, "r", encoding="utf-8") as f:
            content = f.read()

        # [ìˆ˜ì • 2] ìƒˆë¡œìš´ ìƒìˆ˜ ì´ë¦„(LEVEL1_CODE ë“±)ì— ë§ëŠ” ì •ê·œí‘œí˜„ì‹ ì •ì˜
        patterns = {
            "agent_tabular.py": r'export const LEVEL1_CODE = `([\s\S]*?)`;',
            "agent_dqn.py": r'export const LEVEL2_CODE = `([\s\S]*?)`;',
            "agent_ppo.py": r'export const LEVEL3_CODE = `([\s\S]*?)`;',
            "requirements.txt": r'export const REQUIREMENTS_TXT = `([\s\S]*?)`;',
            "run_guide.md": r'export const RUN_GUIDE_MD = `([\s\S]*?)`;'
        }

        for filename, regex in patterns.items():
            match = re.search(regex, content)
            if match:
                code_content = match.group(1)
                extracted_docs.append(Document(
                    page_content=code_content,
                    metadata={"source": f"{filename} (Virtual)"}
                ))
                logger.info(f"   âœ… constants.tsì—ì„œ '{filename}' ì¶”ì¶œ ì™„ë£Œ")
            else:
                logger.warning(f"   âš ï¸ '{filename}' íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        logger.error(f"âŒ constants.ts íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")

    return extracted_docs

def load_and_index_data():
    global retriever
    logger.info("ğŸ”„ ë¬¸ì„œ ë¡œë”© ë° ì¸ë±ì‹± ì‹œì‘...")

    documents = []

    # 1. ê°€ìƒ íŒŒì¼ ë¡œë“œ (constants.ts íŒŒì‹±)
    virtual_docs = extract_from_constants_ts()
    documents.extend(virtual_docs)

    # 2. ì‹¤ì œ ë¬¼ë¦¬ íŒŒì¼ ë¡œë“œ (README.md ë“±)
    physical_files = ["README.md"] # í•„ìš”í•œ íŒŒì¼ ì¶”ê°€
    for relative_path in physical_files:
        full_path = os.path.join(DATA_PATH, relative_path)
        if os.path.exists(full_path):
            try:
                loader = TextLoader(full_path, encoding='utf-8')
                docs = loader.load()
                documents.extend(docs)
                logger.info(f"   âœ… íŒŒì¼ ë¡œë“œ ì„±ê³µ: {relative_path}")
            except Exception as e:
                logger.warning(f"   âš ï¸ ë¡œë“œ ì‹¤íŒ¨ ({relative_path}): {e}")

    if not documents:
        logger.error("âŒ í•™ìŠµí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 3. ë¶„í•  (Chunking)
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separators=["\nclass ", "\ndef ", "\n\n", "\n", " ", ""]
    )
    splits = text_splitter.split_documents(documents)

    # 4. ì„ë² ë”© & ì €ì¥ (base_url ì œê±° -> í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    embedding = OllamaEmbeddings(model=EMBEDDING_MODEL)
    
    vectorstore = Chroma.from_documents(
        documents=splits,
        embedding=embedding,
        collection_name="rl_class_codebase"
    )
    
    retriever = vectorstore.as_retriever(search_kwargs={"k": 3})
    logger.info("ğŸ‰ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ!")

@app.on_event("startup")
async def startup_event():
    try:
        load_and_index_data()
    except Exception as e:
        logger.error(f"âŒ RAG ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class ChatRequest(BaseModel):
    message: str
    history: List[Dict[str, Any]] = []

@app.post("/chat")
async def chat_endpoint(request: ChatRequest):
    if not retriever:
        raise HTTPException(status_code=503, detail="RAG Not Ready")

    # LLM ì´ˆê¸°í™” (base_url ì œê±° -> í™˜ê²½ë³€ìˆ˜ ì‚¬ìš©)
    llm = ChatOllama(model=LLM_MODEL, temperature=0.7)

    template = """
    ë‹¹ì‹ ì€ ê°•í™”í•™ìŠµ ì‹¤ìŠµ ìˆ˜ì—…ì˜ AI ì¡°êµì…ë‹ˆë‹¤.
    ì•„ë˜ [Context]ëŠ” ì´ í”„ë¡œì íŠ¸ì˜ ì†ŒìŠ¤ì½”ë“œì™€ ë¬¸ì„œì…ë‹ˆë‹¤.
    ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

    [Context]
    {context}

    [Question]
    {question}
    
    ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”.
    """
    
    prompt = ChatPromptTemplate.from_template(template)

    rag_chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    try:
        response = rag_chain.invoke(request.message)
        return {"reply": response}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))