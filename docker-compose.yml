services:
  # 1. Frontend Web Server (React)
  web:
    build: .
    ports:
      - "3000:3000"
    environment:
      # [변경] 이제 프론트엔드는 백엔드(8000번)로 요청을 보냅니다.
      - VITE_API_BASE_URL=http://backend:8000
    depends_on:
      - backend
    develop:
      watch:
        - action: sync
          path: ./src
          target: /app/src
          ignore:
            - node_modules/
        - action: rebuild
          path: package.json
    networks:
      - rl-network

  # 2. Backend API Server (FastAPI + RAG) [신규]
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    volumes:
      # [핵심] 호스트의 현재 폴더(./)를 컨테이너의 /app/data에 마운트합니다.
      # 그래야 백엔드가 requirements.txt나 agent_template.py를 읽어서 학습할 수 있습니다.
      - ./:/app/data
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - ANONYMIZED_TELEMETRY=False
    depends_on:
      - ollama
    networks:
      - rl-network

  # 3. Local LLM Server (Ollama)
  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - rl-network
    # [변경] 임베딩 모델(nomic-embed-text) 추가 다운로드
    entrypoint: /bin/sh
    command: -c "ollama serve & sleep 5 && ollama pull llama3.1 && ollama pull nomic-embed-text && wait"

  # 4. Cloudflare Tunnel
  tunnel:
    image: cloudflare/cloudflared:latest
    command: tunnel --url http://web:3000
    networks:
      - rl-network
    depends_on:
      - web

volumes:
  ollama_storage:

networks:
  rl-network:
    driver: bridge